{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "crucial-match",
   "metadata": {},
   "source": [
    "# 507 Project\n",
    "### Author: Diana Liang\n",
    "### Date: 4/15/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-opening",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.utils\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-necklace",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2015 Data --------------------------------------------------------------------------------------\n",
    "\n",
    "# Consumer Data\n",
    "consumer2015 = pd.read_sas('consumer_behavior_2015.XPT')\n",
    "consumer2015 = pd.DataFrame({'SEQN': consumer2015.SEQN, 'money_grocery': consumer2015.CBD071,\n",
    "                            'money_nonfood': consumer2015.CBD091, 'money_food': consumer2015.CBD111,\n",
    "                            'money_eatout': consumer2015.CBD121, 'money_deliver': consumer2015.CBD131})\n",
    "consumer2015.money_grocery.replace(77777, np.nan)\n",
    "consumer2015.money_nonfood.replace(77777, np.nan)\n",
    "consumer2015.money_food.replace(77777, np.nan)\n",
    "consumer2015.money_eatout.replace(77777, np.nan)\n",
    "consumer2015.money_deliver.replace(77777, np.nan)\n",
    "consumer2015.money_grocery.replace(999999, np.nan)\n",
    "consumer2015.money_nonfood.replace(999999, np.nan)\n",
    "consumer2015.money_food.replace(999999, np.nan)\n",
    "consumer2015.money_eatout.replace(999999, np.nan)\n",
    "consumer2015.money_deliver.replace(999999, np.nan)\n",
    "# Health Data\n",
    "health2015 = pd.read_sas('health_2015.XPT')\n",
    "health2015 = pd.DataFrame({'SEQN': health2015.SEQN, 'health': health2015.HSD010})\n",
    "health2015.health.replace(7, np.nan)\n",
    "health2015.health.replace(9, np.nan)\n",
    "# Diet Data\n",
    "diet2015 = pd.read_sas('diet_behavior_2015.XPT')\n",
    "diet2015 = pd.DataFrame({'SEQN': diet2015.SEQN, 'meals_nothome': diet2015.DBD895,\n",
    "                        'meals_fastfood': diet2015.DBD900, 'meals_ready': diet2015.DBD905,\n",
    "                        'meals_frozen': diet2015.DBD910})\n",
    "diet2015.meals_nothome.replace(7777, np.nan)\n",
    "diet2015.meals_fastfood.replace(7777, np.nan)\n",
    "diet2015.meals_ready.replace(7777, np.nan)\n",
    "diet2015.meals_frozen.replace(7777, np.nan)\n",
    "diet2015.meals_nothome.replace(9999, np.nan)\n",
    "diet2015.meals_fastfood.replace(9999, np.nan)\n",
    "diet2015.meals_ready.replace(9999, np.nan)\n",
    "diet2015.meals_frozen.replace(9999, np.nan)\n",
    "# Combine to 2015 data\n",
    "data2015 = pd.merge(health2015, consumer2015, on='SEQN')\n",
    "data2015 = pd.merge(data2015, diet2015, on='SEQN')\n",
    "# Get rid of SEQN\n",
    "del data2015['SEQN']\n",
    "\n",
    "\n",
    "# 2013 Data -----------------------------------------------------------------------------------\n",
    "\n",
    "# Consumer Data\n",
    "consumer2013 = pd.read_sas('consumer_behavior_2013.XPT')\n",
    "consumer2013 = pd.DataFrame({'SEQN': consumer2013.SEQN, 'money_grocery': consumer2013.CBD070,\n",
    "                            'money_nonfood': consumer2013.CBD090, 'money_food': consumer2013.CBD110,\n",
    "                            'money_eatout': consumer2013.CBD120, 'money_deliver': consumer2013.CBD130})\n",
    "consumer2013.money_grocery.replace(77777, np.nan)\n",
    "consumer2013.money_nonfood.replace(77777, np.nan)\n",
    "consumer2013.money_food.replace(77777, np.nan)\n",
    "consumer2013.money_eatout.replace(77777, np.nan)\n",
    "consumer2013.money_deliver.replace(77777, np.nan)\n",
    "consumer2013.money_grocery.replace(999999, np.nan)\n",
    "consumer2013.money_nonfood.replace(999999, np.nan)\n",
    "consumer2013.money_food.replace(999999, np.nan)\n",
    "consumer2013.money_eatout.replace(999999, np.nan)\n",
    "consumer2013.money_deliver.replace(999999, np.nan)\n",
    "# Health Data\n",
    "health2013 = pd.read_sas('health_2013.XPT')\n",
    "health2013 = pd.DataFrame({'SEQN': health2013.SEQN, 'health': health2013.HSD010})\n",
    "health2013.health.replace(7, np.nan)\n",
    "health2013.health.replace(9, np.nan)\n",
    "# Diet Data\n",
    "diet2013 = pd.read_sas('diet_behavior_2013.XPT')\n",
    "diet2013 = pd.DataFrame({'SEQN': diet2013.SEQN, 'meals_nothome': diet2013.DBD895,\n",
    "                        'meals_fastfood': diet2013.DBD900, 'meals_ready': diet2013.DBD905,\n",
    "                        'meals_frozen': diet2013.DBD910})\n",
    "diet2013.meals_nothome.replace(7777, np.nan)\n",
    "diet2013.meals_fastfood.replace(7777, np.nan)\n",
    "diet2013.meals_ready.replace(7777, np.nan)\n",
    "diet2013.meals_frozen.replace(7777, np.nan)\n",
    "diet2013.meals_nothome.replace(9999, np.nan)\n",
    "diet2013.meals_fastfood.replace(9999, np.nan)\n",
    "diet2013.meals_ready.replace(9999, np.nan)\n",
    "diet2013.meals_frozen.replace(9999, np.nan)\n",
    "# Combine to 2015 data\n",
    "data2013 = pd.merge(health2013, consumer2013, on='SEQN')\n",
    "data2013 = pd.merge(data2013, diet2013, on='SEQN')\n",
    "# Get rid of SEQN\n",
    "del data2013['SEQN']\n",
    "\n",
    "\n",
    "# 2011 Data --------------------------------------------------------------------------\n",
    "\n",
    "# Consumer Data\n",
    "consumer2011 = pd.read_sas('consumer_behavior_2011.XPT')\n",
    "consumer2011 = pd.DataFrame({'SEQN': consumer2011.SEQN, 'money_grocery': consumer2011.CBD070,\n",
    "                            'money_nonfood': consumer2011.CBD090, 'money_food': consumer2011.CBD110,\n",
    "                            'money_eatout': consumer2011.CBD120, 'money_deliver': consumer2011.CBD130})\n",
    "consumer2011.money_grocery.replace(77777, np.nan)\n",
    "consumer2011.money_nonfood.replace(77777, np.nan)\n",
    "consumer2011.money_food.replace(77777, np.nan)\n",
    "consumer2011.money_eatout.replace(77777, np.nan)\n",
    "consumer2011.money_deliver.replace(77777, np.nan)\n",
    "consumer2011.money_grocery.replace(999999, np.nan)\n",
    "consumer2011.money_nonfood.replace(999999, np.nan)\n",
    "consumer2011.money_food.replace(999999, np.nan)\n",
    "consumer2011.money_eatout.replace(999999, np.nan)\n",
    "consumer2011.money_deliver.replace(999999, np.nan)\n",
    "# Health Data\n",
    "health2011 = pd.read_sas('health_2011.XPT')\n",
    "health2011 = pd.DataFrame({'SEQN': health2011.SEQN, 'health': health2011.HSD010})\n",
    "health2011.health.replace(7, np.nan)\n",
    "health2011.health.replace(9, np.nan)\n",
    "# Diet Data\n",
    "diet2011 = pd.read_sas('diet_behavior_2011.XPT')\n",
    "diet2011 = pd.DataFrame({'SEQN': diet2011.SEQN, 'meals_nothome': diet2011.DBD895,\n",
    "                        'meals_fastfood': diet2011.DBD900, 'meals_ready': diet2011.DBD905,\n",
    "                        'meals_frozen': diet2011.DBD910})\n",
    "diet2011.meals_nothome.replace(7777, np.nan)\n",
    "diet2011.meals_fastfood.replace(7777, np.nan)\n",
    "diet2011.meals_ready.replace(7777, np.nan)\n",
    "diet2011.meals_frozen.replace(7777, np.nan)\n",
    "diet2011.meals_nothome.replace(9999, np.nan)\n",
    "diet2011.meals_fastfood.replace(9999, np.nan)\n",
    "diet2011.meals_ready.replace(9999, np.nan)\n",
    "diet2011.meals_frozen.replace(9999, np.nan)\n",
    "# Combine to 2015 data\n",
    "data2011 = pd.merge(health2011, consumer2011, on='SEQN')\n",
    "data2011 = pd.merge(data2011, diet2011, on='SEQN')\n",
    "# Get rid of SEQN\n",
    "del data2011['SEQN']\n",
    "\n",
    "\n",
    "# Merge 3 years ---------------------------------------------------------------------\n",
    "\n",
    "data = data2015.append(data2013, ignore_index=True)\n",
    "data = data.append(data2011, ignore_index=True)\n",
    "data = data.query('health!=\"NaN\"')\n",
    "\n",
    "data.to_csv('food_health.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-exposure",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_health = pd.read_csv('food_health.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-affect",
   "metadata": {},
   "source": [
    "Create training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out how many to include\n",
    "num_obs = len(food_health.health)\n",
    "num_train = int((num_obs*0.8)//1)\n",
    "train_idx = random.sample(list(range(num_obs)), num_train)\n",
    "#food_health.loc[train_idx]\n",
    "test_idx = []\n",
    "for i in range(num_obs):\n",
    "    if i not in train_idx:\n",
    "        test_idx.append(i)\n",
    "print(len(train_idx), len(test_idx))\n",
    "\n",
    "# Create data sets\n",
    "train = food_health.loc[train_idx]\n",
    "test = food_health.loc[test_idx]\n",
    "train.to_csv('food_train.csv')\n",
    "test.to_csv('food_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-boxing",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at distribution of health\n",
    "data = pd.read_csv('food_health.csv')\n",
    "health_count = {}\n",
    "health_count['Excellent'] = np.sum(data['health'] == 1)\n",
    "health_count['Very good'] = np.sum(data['health'] == 2)\n",
    "health_count['Good'] = np.sum(data['health'] == 3)\n",
    "health_count['Fair'] = np.sum(data['health'] == 4)\n",
    "health_count['Poor'] = np.sum(data['health'] == 5)\n",
    "plt.bar(health_count.keys(), health_count.values())\n",
    "plt.title(\"Distribution of perceived health\")\n",
    "plt.xlabel(\"Perceived health\")\n",
    "plt.ylabel(\"# Respondents\")\n",
    "plt.show()\n",
    "health_count\n",
    "\n",
    "# Check how many missing data\n",
    "num_missing = {}\n",
    "i = 0\n",
    "del data['health']\n",
    "for x in data:\n",
    "    i += 1\n",
    "    num_missing[x] = sum(np.isnan(data[x]))\n",
    "plt.bar(num_missing.keys(), num_missing.values())\n",
    "plt.title(\"Number of missing data values per predictor\")\n",
    "plt.xlabel(\"Predictor Variables\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"# missing values\")\n",
    "plt.show()\n",
    "sum(np.isnan(data['meals_fastfood']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-pendant",
   "metadata": {},
   "source": [
    "Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training and Validation data sets\n",
    "X = data.iloc[:, 1:]\n",
    "Y = data.iloc[:, 0]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1,\n",
    "                                                   random_state=0)\n",
    "miss_train = IterativeImputer(max_iter=100, random_state=0)\n",
    "new_X_train = miss_train.fit_transform(X_train)\n",
    "miss_test = IterativeImputer(max_iter=100, random_state=0)\n",
    "new_X_test = miss_test.fit_transform(X_test)\n",
    "\n",
    "# Run random forests\n",
    "oob = []\n",
    "for each_num in [1000, 5000, 7500]:\n",
    "    for each_max in [2, 3, 4, 5]:\n",
    "        tree = RandomForestClassifier(n_estimators=each_num, max_features=each_max, oob_score = True, random_state=0)\n",
    "        tree.fit(new_X_train, Y_train)\n",
    "        oob.append(tree.oob_score_)\n",
    "        print('tree:', each_num, 'feat:', each_max)\n",
    "\n",
    "# Visualize\n",
    "\n",
    "## Accuracy vs Parameters\n",
    "plt.plot(oob)\n",
    "plt.title(\"Average Accuracy for Different Parameters\")\n",
    "plt.xlabel(\"Different Parameters\")\n",
    "plt.ylabel(\"OOB Score\")\n",
    "#plt.savefig('oob.png')\n",
    "\n",
    "\n",
    "## Accuracy vs num Predictors\n",
    "plt.plot([2, 3, 4, 5], oob[4:8])\n",
    "plt.title(\"Average Accuracy vs. # of Predictors for 500 Estimators\")\n",
    "plt.xlabel(\"# of Predictors\")\n",
    "#plt.ylabel(\"OOB Score\")\n",
    "\n",
    "## Accuracy\n",
    "model = RandomForestClassifier(n_estimators=5000, max_features=3, oob_score = True, random_state=0)\n",
    "model.fit(new_X_train, Y_train)\n",
    "Y_pred = model.predict(new_X_test)\n",
    "accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "## Prediction Heatmap\n",
    "sns.heatmap(confusion_matrix(Y_test, Y_pred), xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5], annot=True)\n",
    "plt.title(\"Predicted vs Actual Health Confusion Matrix\")\n",
    "plt.xlabel(\"Actual Health\")\n",
    "plt.ylabel(\"Predicted Health\")\n",
    "#plt.savefig(\"con_mat.png\")\n",
    "\n",
    "## Variable Importance\n",
    "feat_import = pd.DataFrame({'Features': list(X.columns),\n",
    "                            'Score': list(model.feature_importances_)}).sort_values(by='Score', ascending=False)\n",
    "sns.barplot(x=feat_import.Features, y=feat_import.Score)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.xticks(rotation=75)\n",
    "#plt.savefig(\"var_imp.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-applicant",
   "metadata": {},
   "source": [
    "Simplified random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify health categories to healthy or not healthy\n",
    "data = pd.read_csv('food_health.csv')\n",
    "data['is_1'] = data.health == 1\n",
    "data['is_2'] = data.health == 2\n",
    "data['is_3'] = data.health == 3\n",
    "data['is_healthy'] = data.is_1 + data.is_2 + data.is_3\n",
    "\n",
    "del data['health']\n",
    "del data['is_1']\n",
    "del data['is_2']\n",
    "del data['is_3']\n",
    "\n",
    "# Create Training and Validation data sets\n",
    "X = data.iloc[:, :9]\n",
    "Y = data.iloc[:, 9]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1,\n",
    "                                                   random_state=0)\n",
    "miss_train = IterativeImputer(max_iter=100, random_state=0)\n",
    "new_X_train = miss_train.fit_transform(X_train)\n",
    "miss_test = IterativeImputer(max_iter=100, random_state=0)\n",
    "new_X_test = miss_test.fit_transform(X_test)\n",
    "\n",
    "# Run random forests\n",
    "s_oob = []\n",
    "for each_num in [500, 1000, 5000]:\n",
    "    for each_max in [2, 3, 4, 5]:\n",
    "        tree = RandomForestClassifier(n_estimators=each_num,\n",
    "                                      max_features=each_max,\n",
    "                                      oob_score = True,\n",
    "                                      random_state=0)\n",
    "        tree.fit(new_X_train, Y_train)\n",
    "        s_oob.append(tree.oob_score_)\n",
    "        print('feat:', each_max, 'num:', each_num)\n",
    "        \n",
    "# Visualization\n",
    "\n",
    "## Accuracy vs Parameters\n",
    "plt.plot(s_oob)\n",
    "plt.title(\"Average Accuracy for Different Parameters (Simple Model)\")\n",
    "plt.xlabel(\"Different Parameters\")\n",
    "plt.ylabel(\"OOB Score\")\n",
    "#plt.savefig('simple_oob.png')\n",
    "\n",
    "plt.plot([2, 3, 4, 5], s_oob[8:])\n",
    "s_oob[7:]\n",
    "\n",
    "## Accuracy\n",
    "model = RandomForestClassifier(n_estimators=5000,\n",
    "                               max_features=2,\n",
    "                               oob_score = True,\n",
    "                               random_state=0)\n",
    "model.fit(new_X_train, Y_train)\n",
    "Y_pred = model.predict(new_X_test)\n",
    "accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "## Prediction Heatmap\n",
    "sns.heatmap(confusion_matrix(Y_test, Y_pred), annot=True)\n",
    "plt.title(\"Predicted vs Actual Health Confusion Matrix (Simple Model)\")\n",
    "plt.xlabel(\"Actual Health\")\n",
    "plt.ylabel(\"Predicted Health\")\n",
    "#plt.savefig(\"simple_con_mat.png\")\n",
    "\n",
    "## Variable Importance\n",
    "feat_import = pd.DataFrame({'Features': list(X.columns),\n",
    "                            'Score': list(model.feature_importances_)}).sort_values(by='Score', ascending=False)\n",
    "sns.barplot(x=feat_import.Features, y=feat_import.Score)\n",
    "plt.title(\"Feature Importance (Simple Model)\")\n",
    "plt.xticks(rotation=90)\n",
    "#plt.savefig(\"simple_var_imp.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-substance",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Training and Validation data sets\n",
    "data = pd.read_csv('food_health.csv')\n",
    "X = data.iloc[:, 1:]\n",
    "Y = data.iloc[:, 0]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1,\n",
    "                                                   random_state=0)\n",
    "miss_train = IterativeImputer(max_iter=100, random_state=0)\n",
    "new_X_train = miss_train.fit_transform(X_train)\n",
    "miss_test = IterativeImputer(max_iter=100, random_state=0)\n",
    "new_X_test = miss_test.fit_transform(X_test)\n",
    "\n",
    "# Run logistic regression\n",
    "train_acc = []\n",
    "for c in [0.01, 0.1, 1, 10]:\n",
    "    model = LogisticRegression(C=c, multi_class='multinomial', max_iter=5000, random_state=0)\n",
    "    scores = cross_val_score(model, new_X_train, Y_train, scoring='accuracy',\n",
    "                            cv=8)\n",
    "    train_acc.append(scores.mean())\n",
    "\n",
    "    \n",
    "# Visualize\n",
    "\n",
    "## Accuracy vs Parameters\n",
    "plt.plot(range(1,5), train_acc)\n",
    "plt.title(\"Average Accuracy for Different Parameters (Simple LR)\")\n",
    "plt.xlabel(\"Different Parameters\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "#plt.savefig('lr_acc.png')\n",
    "\n",
    "## Accuracy\n",
    "model = LogisticRegression(C=1, multi_class='multinomial',\n",
    "                           max_iter=5000, random_state=0)\n",
    "model.fit(new_X_train, Y_train)\n",
    "Y_pred = model.predict(new_X_test)\n",
    "accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "## Prediction Heatmap\n",
    "sns.heatmap(confusion_matrix(Y_test, Y_pred), annot=True)\n",
    "plt.title(\"Predicted vs Actual Health Confusion Matrix (Simple LR)\")\n",
    "plt.xlabel(\"Actual Health\")\n",
    "plt.ylabel(\"Predicted Health\")\n",
    "#plt.savefig(\"lr_con_mat.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-dividend",
   "metadata": {},
   "source": [
    "Simplified Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode and set up Training and Validation data sets\n",
    "data = pd.read_csv('food_health.csv')\n",
    "data['is_1'] = data.health == 1\n",
    "data['is_2'] = data.health == 2\n",
    "data['is_3'] = data.health == 3\n",
    "data['is_healthy'] = data.is_1 + data.is_2 + data.is_3\n",
    "del data['health']\n",
    "del data['is_1']\n",
    "del data['is_2']\n",
    "del data['is_3']\n",
    "X = data.iloc[:, :9]\n",
    "Y = data.iloc[:, 9]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1,\n",
    "                                                   random_state=0)\n",
    "miss_train = IterativeImputer(max_iter=100, random_state=0)\n",
    "new_X_train = miss_train.fit_transform(X_train)\n",
    "miss_test = IterativeImputer(max_iter=100, random_state=0)\n",
    "new_X_test = miss_test.fit_transform(X_test)\n",
    "\n",
    "# Run logistic regression\n",
    "train_acc = []\n",
    "for c in [0.01, 0.1, 1, 10]:\n",
    "    model = LogisticRegression(C=c, multi_class='ovr', max_iter=5000, random_state=0)\n",
    "    scores = cross_val_score(model, new_X_train, Y_train, scoring='accuracy',\n",
    "                            cv=8)\n",
    "    train_acc.append(scores.mean())\n",
    "    \n",
    "# Visualization\n",
    "\n",
    "## Accuracy vs Parameters\n",
    "plt.plot(range(1,5), train_acc)\n",
    "plt.title(\"Average Accuracy for Different Parameters (Simple LR)\")\n",
    "plt.xlabel(\"Different Parameters\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "#plt.savefig('lr_acc.png')\n",
    "\n",
    "## Accuracy\n",
    "model = LogisticRegression(C=0.1, multi_class='ovr',\n",
    "                           max_iter=5000, random_state=0)\n",
    "model.fit(new_X_train, Y_train)\n",
    "Y_pred = model.predict(new_X_test)\n",
    "accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "## Prediction Heatmap\n",
    "sns.heatmap(confusion_matrix(Y_test, Y_pred), annot=True)\n",
    "plt.title(\"Predicted vs Actual Health Confusion Matrix (Simple LR)\")\n",
    "plt.xlabel(\"Predicted Health\")\n",
    "plt.ylabel(\"Actual Health\")\n",
    "#plt.savefig(\"lr_con_mat.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-brick",
   "metadata": {},
   "source": [
    "Simplified MLP (Multi-Level Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode and set up Training and Validation data sets\n",
    "data = pd.read_csv('food_health.csv')\n",
    "data['is_1'] = data.health == 1\n",
    "data['is_2'] = data.health == 2\n",
    "data['is_3'] = data.health == 3\n",
    "data['is_healthy'] = data.is_1 + data.is_2 + data.is_3\n",
    "del data['health']\n",
    "del data['is_1']\n",
    "del data['is_2']\n",
    "del data['is_3']\n",
    "X = data.iloc[:, :9]\n",
    "Y = data.iloc[:, 9]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1,\n",
    "                                                   random_state=0)\n",
    "miss_train = IterativeImputer(max_iter=100, random_state=0)\n",
    "new_X_train = miss_train.fit_transform(X_train)\n",
    "miss_test = IterativeImputer(max_iter=100, random_state=0)\n",
    "new_X_test = miss_test.fit_transform(X_test)\n",
    "\n",
    "# Run MLP\n",
    "train_acc = []\n",
    "for unit in [50, 100, 1000]:\n",
    "    for act in ['relu', 'elu', 'tanh', 'sigmoid']:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(unit, input_dim=9, activation=act))\n",
    "        model.add(Dense(10, activation=act))\n",
    "        model.add(Dense(1, activation='softmax'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(new_X_train, Y_train, epochs=10, batch_size=32)\n",
    "        _, acc = model.evaluate(new_X_train, Y_train)\n",
    "        train_acc.append(acc)\n",
    "        \n",
    "# Visualization\n",
    "\n",
    "## Accuracy vs Parameters\n",
    "plt.title(\"Accuracy for Different Parameters (Simple MLP)\")\n",
    "plt.xlabel(\"Different Parameters\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(train_acc)\n",
    "#plt.savefig('mlp_acc.png')\n",
    "\n",
    "## Run with best parameters and accuracy\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=9, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(new_X_train, Y_train, epochs=10, batch_size=32)\n",
    "Y_pred = model.predict_classes(new_X_test)\n",
    "accuracy_score(Y_pred, Y_test)\n",
    "\n",
    "## Prediction Heatmap\n",
    "sns.heatmap(confusion_matrix(Y_test, Y_pred), annot=True)\n",
    "plt.title(\"Predicted vs Actual Health Confusion Matrix (Simple MLP)\")\n",
    "plt.xlabel(\"Predicted Health\")\n",
    "plt.ylabel(\"Actual Health\")\n",
    "#plt.savefig(\"mlp_con_mat.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
